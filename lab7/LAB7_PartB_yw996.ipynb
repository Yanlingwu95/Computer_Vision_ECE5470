{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> LAB 7: Machine Learning using the Scikit-learn and the MNIST dataset </center>\n",
    "## <center> Part B   Experiments with Classifiers</center>\n",
    "# <center> Yanling Wu </center>\n",
    "# <center> yw996 </center>\n",
    "\n",
    "## **1: K-Neraest-Neighbor (KNN)** \n",
    " \n",
    "In this section, we will evaluate KNN classifier performance and explore how to improve its performance. \n",
    "\n",
    "### Different Size of Dataset\n",
    "\n",
    "\n",
    "In order to control the computing time with 10 minutes, I tried to use *StandardScaler* to standardlize the dataset, it did decrease the computing time, but the prediction accuracy decreased more. Hence, I chose to narrow the size of dataset and tried different sizes of training set and test set and we need to take the running time and predicted accuracy into consideration. Intuitively, decreasing the size of training dataset will lead to the decrease of prediction accuracy. Shown as below figures, we can see the prediction score increasd as the datasize increases. \n",
    "\n",
    "After several attempt, I set the training set to 40,000 and the test set to 6,000, which can guarantee the running time with 10 minutes and the accuracy is not that low. \n",
    "\n",
    "<center>\n",
    "    <img src=\"./pic/1w_2k_3nn.png\" width=\"50%\" height=\"50%\" />  \n",
    "    Figure 1.  10k training dataset and 2k test dataset\n",
    "    <img src=\"./pic/2w_4k_3nn.png\" width=\"50%\" height=\"50%\" />\n",
    "    Figure 2.20k training dataset and 4k test dataset\n",
    "    <img src=\"./pic/3w_6k_3nn.png\" width=\"50%\" height=\"50%\" />\n",
    "    Figure 3.  30k training dataset and 6k test dataset\n",
    "    <img src=\"./pic/4w_6k_3nn.png\" width=\"50%\" height=\"50%\" />\n",
    "    Figure 4.  40k training dataset and 6k test dataset\n",
    "</center>\n",
    "\n",
    "### Different Number of Near Neighbor\n",
    "\n",
    "Next, I tried different number of near neighbors from 1 to 5. The following figure shows the result. Compared 1NN with 3NN, the accuracy of 1NN is bigger than that of 3NN. And the change trend from 1 to 5 near neighbors is down, up and down. The good amount of near neighbors is one or four. But there are higher possibility for 1NN to overfit the data since it needs to consider all of the near neighbors during fitting the model. \n",
    "\n",
    "<center>\n",
    "<img src=\"./pic/4w_6k_1-6nn.png\" width=\"60%\" height=\"60%\" />    \n",
    "Figure 5. The result of different number of near neighboring \n",
    "</center>\n",
    " \n",
    " \n",
    "## **2: Multi-Layer Perceptron (MLP)** \n",
    "\n",
    "In this section, we used the MLP classifier to recognize the digits and compared the results of model with one MLP layer to that of model with two MLP layers. I ran the program in Google Colaboratory whose computation speed is very fast. So I used all minist dataset whose total amount is 70,000 and 10,000 of them are as test set. In order to optimal the MLP classifier to get the better score of prediction, we need to change some vital parameters of MLPClassifier including *learning_rate_init, max_iter, alpha, tol, alpha, solver*. \n",
    "\n",
    "Specificly, the parameter of *learning_rate_init* will control the size of every step to calculate, which can affect the score of prediction hugely and I changed this parameters to 0.0001. Besides, the parameter of *max_iter* will decide the number of epoches and the times of iterations and will influence the length of calculation. Since we need to control the length of computation of this program within 10 minutes, I changed it to 300 steps. Additionally, the parameter of *tol* will give a criterion to end the calculation and optimization, meaning that when the decreasing value of the loss between every calculation is more than value of *tol*, the training will be ended. The parameter of *solver* will decide what mathematic function and method will be used to train the classifier. \n",
    "\n",
    "### MLP Classifier with One Layer\n",
    "\n",
    "After every iteration, the loss will decrease and I set the number of iteration to 300 and we can see the loss value of last iteration have already decreased to 0.097 and taining time is 372 seconds. The score of prediction of training set is 0.9709 and the score of predicting of test set is 0.9445. This result is not bad. \n",
    "<center>\n",
    "<img src=\"./pic/MLP_1layer_1.png\" width=\"70%\" height=\"70%\" />    \n",
    "Figure 6.the Result of MLP Classifier with One Layer\n",
    "</center>\n",
    "\n",
    "### MLP Classifier with two Layers\n",
    "\n",
    "In this step, we add one more layer to recognize the digits and try to get better score of prediction. It took 453 seconds to iterate 300 times to calculate two layers model and the loss value of last iteration have declined to 0.067 and the score of prediction of training set is 0.981467 and the score of predicting of test set is 0.9457, which is better than the one layer model, especially the socre of training set. \n",
    "\n",
    "<center>\n",
    "<img src=\"./pic/MLP_2layer_renum.png\" width=\"70%\" height=\"70%\" />    \n",
    "Figure 7.the Result of MLP Classifier with One Layer\n",
    "</center>\n",
    "\n",
    "                    \n",
    "  \n",
    "## **3: Support Vector Machine (SVM)** \n",
    "\n",
    "Support vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.And we could choose different kernel of classifiers to improve the performance of it. Besides linear classificaiton, there are several non-linear classification method.\n",
    "\n",
    "In this section, we will apply three kinds of classifiers and compare their performances and control their running time with 10 minutes. \n",
    "\n",
    "#### SVM with a linear funciton \n",
    "We tried to build a SVM Classifiers with a linear function using 30,000 training data and 6000 test data.Training time is 308.556 seconds and the accuracy is 0.9038, which is not so high. \n",
    "\n",
    "<center>\n",
    "<img src=\"./pic/linear.png\" width=\"70%\" height=\"70%\" />    \n",
    "Figure 8.the Result of SVM Classifier with Linear function\n",
    "</center>\n",
    "\n",
    "#### SVM with a polynomial funciton\n",
    "We tried to build a SVM Classifiers with a polynomial function using 30,000 training data and 6000 test data. Training time is 177.658 seconds and the accuracy is 0.971667, which is far higher than that of classifier with a linear funtion.\n",
    "\n",
    "<center>\n",
    "<img src=\"./pic/poly.png\" width=\"70%\" height=\"70%\" />    \n",
    "Figure 9.the Result of SVM Classifier with Polynomial function\n",
    "</center>\n",
    "\n",
    "#### SVM with radial basis function\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./pic/poly.png\" width=\"70%\" height=\"70%\" />    \n",
    "Figure 10.the Result of SVM Classifier with radial basis function\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
